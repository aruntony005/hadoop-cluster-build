
On all the nodes
===========================

mkdir .ssh

cat > .ssh/id_rsa << EOF
-----BEGIN OPENSSH PRIVATE KEY-----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-----END OPENSSH PRIVATE KEY-----
EOF


cat > .ssh/id_rsa.pub << EOF
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC14Y4hzCySZzguUI5GnRK0A1odTQCf2X47in3PThFkISYiH2DjMvBcMjdyyvg3R2MeTjQVloOi0Xd4yNAGhPc2XDBQFRWaBMKpnvoHKC13XyqpoolnaMxSXHkpFB1b4ZxHqorVhGqwtEwW8Iaj72YTrkbDEj5cwJCIoC1H/o57JmzOJKcMBXRGcjZZkfTBtrffQ5YTtM66LbjS6HHsxccEO+FOkSW/LWvtV302927CxKKfMuHfpqHBfq3M2Y3Z0yNGaYnIdhQYBS3e7xZTQbP/cntOpXcDPRyyao3JYF96/1epHykUNJqrAslhH1oNJ8LdKeZCw7VgRkJ946Ocy+biXQea9DT3niV1QwO6VMUI73sLPcSOU7qI/A+mqOM+TxbeztYMbuqaBQPXkSAJ/5wEmFfXwJajd2U0Y/6bVbQ6TD+b9zD61MnvOjnrx/7EtrDyfyZhAExwF9UZwURTisYvpk2wjvFouYsTbhqDvCryq40o3ig7xnNRGmJTAlItlUs= home/hdfs@master
EOF

cat > .ssh/authorized_keys << EOF
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC14Y4hzCySZzguUI5GnRK0A1odTQCf2X47in3PThFkISYiH2DjMvBcMjdyyvg3R2MeTjQVloOi0Xd4yNAGhPc2XDBQFRWaBMKpnvoHKC13XyqpoolnaMxSXHkpFB1b4ZxHqorVhGqwtEwW8Iaj72YTrkbDEj5cwJCIoC1H/o57JmzOJKcMBXRGcjZZkfTBtrffQ5YTtM66LbjS6HHsxccEO+FOkSW/LWvtV302927CxKKfMuHfpqHBfq3M2Y3Z0yNGaYnIdhQYBS3e7xZTQbP/cntOpXcDPRyyao3JYF96/1epHykUNJqrAslhH1oNJ8LdKeZCw7VgRkJ946Ocy+biXQea9DT3niV1QwO6VMUI73sLPcSOU7qI/A+mqOM+TxbeztYMbuqaBQPXkSAJ/5wEmFfXwJajd2U0Y/6bVbQ6TD+b9zD61MnvOjnrx/7EtrDyfyZhAExwF9UZwURTisYvpk2wjvFouYsTbhqDvCryq40o3ig7xnNRGmJTAlItlUs= home/hdfs@master
EOF

sudo chmod 700 .ssh
sudo chmod 600 .ssh/id_rsa
sudo chmod 644 .ssh/authorized_keys

#run as root
========================
sudo sed -i '/PasswordAuthentication/d;/PermitRootLogin/d;/PubkeyAuthentication/d' /etc/ssh/sshd_config
sudo echo -e "PermitRootLogin yes\nPubkeyAuthentication yes\nPasswordAuthentication yes" >> /etc/ssh/sshd_config
sudo systemctl restart sshd
========================

sudo yum install java-1.8.0-openjdk-devel wget -y
wget https://downloads.apache.org/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz

sudo chown hdfs:hdfs hadoop-3.2.2.tar.gz
tar -xvf hadoop-3.2.2.tar.gz
sudo mv hadoop-3.2.2 /opt/hadoop

cat >> .bash_profile << EOF
export PATH=\$PATH:/opt/hadoop/bin:/opt/hadoop/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/bin
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.292.b10-1.el8_4.x86_64/jre
export HADOOP_HOME=/opt/hadoop
export PATH=\$PATH:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/bin:/bin:/sbin
export HDFS_NAMENODE_USER="hdfs"
export HDFS_DATANODE_USER="hdfs"
export HDFS_SECONDARYNAMENODE_USER="hdfs"
export YARN_RESOURCEMANAGER_USER="hdfs"
export YARN_NODEMANAGER_USER="hdfs"
EOF

cat >> .bashrc << EOF
export PATH=\$PATH:/opt/hadoop/bin:/opt/hadoop/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/bin
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.292.b10-1.el8_4.x86_64/jre
EOF

source ~/.bashrc
source ~/.bash_profile

echo "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.292.b10-1.el8_4.x86_64/jre" >> /opt/hadoop/etc/hadoop/hadoop-env.sh
echo "export HADOOP_HOME=/opt/hadoop" >> /opt/hadoop/etc/hadoop/hadoop-env.sh

sed -i '/configuration/d' /opt/hadoop/etc/hadoop/core-site.xml

cat >> /opt/hadoop/etc/hadoop/core-site.xml << EOF
<configuration>
       <property>
            <name>fs.default.name</name>
            <value>hdfs://localhost:9000</value>
        </property>
</configuration>
EOF

sed -i '/configuration/d' /opt/hadoop/etc/hadoop/hdfs-site.xml

cat >> /opt/hadoop/etc/hadoop/hdfs-site.xml << EOF
<configuration>
    <property>
            <name>dfs.namenode.name.dir</name>
            <value>/opt/data/nameNode</value>
    </property>
    <property>
            <name>dfs.datanode.data.dir</name>
            <value>/opt/data/dataNode</value>
    </property>
    <property>
            <name>dfs.replication</name>
            <value>2</value>
    </property>
</configuration>
EOF

sed -i '/configuration/d' /opt/hadoop/etc/hadoop/mapred-site.xml

cat >> /opt/hadoop/etc/hadoop/mapred-site.xml << EOF
<configuration>

    <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
    </property>
    <property>
            <name>yarn.app.mapreduce.am.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.map.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.reduce.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>

    <property>
        <name>yarn.app.mapreduce.am.resource.mb</name>
        <value>512</value>
</property>

<property>
        <name>mapreduce.map.memory.mb</name>
        <value>256</value>
</property>

<property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>256</value>
</property>
</configuration>
EOF

sed -i '/configuration/d' /opt/hadoop/etc/hadoop/yarn-site.xml

cat >> /opt/hadoop/etc/hadoop/yarn-site.xml << EOF
<configuration>

<!-- Site specific YARN configuration properties -->

    <property>
            <name>yarn.acl.enable</name>
            <value>0</value>
    </property>

    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>localhost</value>
    </property>

    <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
    </property>

    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>128</value>
</property>

<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
</property>

</configuration>
EOF



# echo -e "worker1\nworker2" >> /opt/hadoop/etc/hadoop/workers
sudo chown -R hdfs:hdfs /opt
=================================================

# Only on 10.128.0.50
========================
hdfs namenode -format

start-all.sh
=======================

-> Get the hadoop namenode UI in 
http://<10.128.0.50 ip>:9870/

######## SPARK #############

https://medium.com/@jootorres_11979/how-to-install-and-set-up-an-apache-spark-cluster-on-hadoop-18-04-b4d70650ed42

On 10.128.0.50 node 
===========================
hdfs dfs -mkdir -p /spark/logs
=============================

On all nodes
============================
wget https://apachemirror.wuchna.com/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz
tar -xzf spark-2.4.8-bin-hadoop2.7.tgz
mv spark-2.4.8-bin-hadoop2.7 /opt/spark


cat >> .bash_profile << EOF
export SPARK_HOME=/opt/spark
export PATH=\$PATH:\$SPARK_HOME/bin

export LD_LIBRARY_PATH=/opt/hadoop/lib/native
export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
export PATH=\$PATH:\$HADOOP_HOME/etc/hadoop:\$HADOOP_HOME/lib/native

export PYTHONPATH=\$SPARK_HOME/python:\$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:\$PYTHONPATH
export PYSPARK_PYTHON=python3
EOF

cat >> .bashrc << EOF
export SPARK_HOME=/opt/spark
export PATH=\$PATH:\$SPARK_HOME/bin
EOF

source ~/.bashrc
source ~/.bash_profile

mv /opt/spark/conf/spark-defaults.conf.template /opt/spark/conf/spark-defaults.conf


cat >> /opt/spark/conf/spark-defaults.conf << EOF
spark.localhost    yarn
spark.driver.memory    512m
spark.yarn.am.memory    512m
spark.executor.memory          512m
spark.eventLog.enabled  true
spark.eventLog.dir hdfs://localhost:9000/spark/logs
spark.history.provider            org.apache.spark.deploy.history.FsHistoryProvider
spark.history.fs.logDirectory     hdfs://localhost:9000/spark/logs
spark.history.fs.update.interval  10s
spark.history.ui.port             18080
EOF

mv /opt/spark/conf/spark-env.sh.template /opt/spark/conf/spark-env.sh

# export SPARK_MASTER_HOST='<10.128.0.50-IP>'export JAVA_HOME=<Path_of_JAVA_installation>

echo "export SPARK_MASTER_HOST='localhost'" >> /opt/spark/conf/spark-env.sh
echo "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.292.b10-1.el8_4.x86_64/jre" >> /opt/spark/conf/spark-env.sh

# mv /opt/spark/conf/slaves.template /opt/spark/conf/slaves
# echo -e "10.128.0.50\nworker1\nworker2" > /opt/spark/conf/slaves
============================

On 10.128.0.50 node 
==================================
sh /opt/spark/sbin/start-all.sh
===================================

-> Get the spark ui in 
http://<10.128.0.50 ip>:8080
-> Get the yarn resource manager in 
http://<10.128.0.50 ip>:8088


-> Test with data
sudo yum install git -y
git clone https://github.com/dgadiraju/data.git
hdfs dfs -mkdir /public
hdfs dfs -put ${HOME}/data/retail_db /public/.

pyspark
>>> a=spark.read.csv('/public/retail_db/orders')
>>> a.show()

######### HIVE ################

-> Refer the below link for install hive with remote metastore database.
https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration
https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-centos-7

-> Install mysql on one node
==========================================
wget https://dev.mysql.com/get/mysql57-community-release-el7-9.noarch.rpm

-> To add mysql yum repo

sudo rpm -ivh mysql57-community-release-el7-9.noarch.rpm
sudo yum install mysql-server -y
sudo systemctl start mysqld
sudo systemctl status mysqld
mysql_secure_installation




-> Now conect to the database
mysql -u root -p
Enter password: aruntonyml

create database hive;

sudo mysql -u root -p -e"create database hive"
sudo mysql -uroot -p -e "CREATE USER 'hiveuser'@'%' IDENTIFIED BY 'hivepassword'"
sudo mysql -uroot -p -e "GRANT ALL PRIVILEGES ON *.* TO 'hiveuser'@'%' WITH GRANT OPTION"

==========================================

on 10.128.0.50 node
======================
hdfs dfs -mkdir -p /user/hive/warehouse
hdfs dfs -mkdir /tmp
hdfs dfs -chmod -R 777 /user/hive/warehouse
hdfs dfs -chmod -R 777 /tmp
===================================

On all nodes 
=====================================
wget https://downloads.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz
tar -xzf apache-hive-3.1.2-bin.tar.gz
mv apache-hive-3.1.2-bin /opt/hive

cat >> ~/.bash_profile << EOF
export HIVE_HOME=/opt/hive
export PATH=\$PATH:\$HIVE_HOME/bin
EOF

cat >> ~/.bashrc << EOF
export HIVE_HOME=/opt/hive
export PATH=\$PATH:\$HIVE_HOME/bin
EOF

source ~/.bashrc
source ~/.bash_profile



mv /opt/hive/conf/hive-env.sh.template /opt/hive/conf/hive-env.sh

cat >> /opt/hive/conf/hive-env.sh << EOF
export HADOOP_HOME=/opt/hadoop
export HIVE_CONF_DIR=/opt/hive/conf
EOF


cat > /opt/hive/conf/hive-site.xml << EOF
<?xml version="1.0"?>
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://localhost:3306/hive</value>
        <description>JDBC connection string used by Hive Metastore</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
        <description>JDBC Driver class</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hiveuser</value>
        <description>Metastore database user name</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>hivepassword</value>
        <description>Metastore database password</description>
    </property>

 <property>
    <name>datanucleus.connectionPoolingType</name>
    <value>dbcp</value>

  </property>
 <property>
    <name>hive.metastore.schema.verification</name>
    <value>true</value>

  </property>

  <property>
    <name>hive.metastore.uris</name>
    <value>thrift://localhost:9083</value>
    <description>URI for client to contact metastore server</description>
  </property>

</configuration>
EOF

wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.24-1.el8.noarch.rpm
sudo rpm -ivh mysql-connector-java-8.0.24-1.el8.noarch.rpm
sudo cp /usr/share/java/mysql-connector-java.jar /opt/hive/lib
cp /opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar /opt/hive/lib
rm /opt/hive/lib/guava-19.0.jar

========spark+hive=========

cp  /opt/hive/conf/hive-site.xml  /opt/spark/conf/

schematool -dbType mysql -initSchema
schematool -dbType mysql -info

nohup hive --service metastore & /dev/null 2>&1


hive

CREATE DATABASE qwerty;

create table qwerty.orders (
  order_id int,
  order_date string,
  order_customer_id int,
  order_status string
) row format delimited fields terminated by ','
stored as textfile;

load data inpath '/public/retail_db/orders' into table qwerty.orders;








========================================================================================================================================

============kafka

sudo yum -y install nmap
wget https://downloads.apache.org/kafka/2.8.0/kafka_2.13-2.8.0.tgz
tar -xzf kafka_2.13-2.8.0.tgz
mv kafka_2.13-2.8.0 kafka


cat >> /opt/.bash_profile << EOF
export KAFKA_HOME=/opt/kafka
export PATH=\$PATH:\$KAFKA_HOME/bin
EOF

cat >> /opt/.bashrc << EOF
export KAFKA_HOME=/opt/kafka
export PATH=\$PATH:\$KAFKA_HOME/bin
EOF

source ~/.bashrc
source ~/.bash_profile



mkdir /opt/zookeeper_data
mkdir /opt/kafka_data

mv /opt/kafka/config/zookeeper.properties /opt/kafka/config/zookeeper.properties.backup

cat >> /opt/kafka/config/zookeeper.properties << EOF
dataDir=/opt/zookeeper_data
clientPort=2181
maxClientCnxns=0
admin.enableServer=false
tickTime=2000
initLimit=10
syncLimit=5

4lw.commands.whitelist=stat, ruok, conf, isro
4lw.commands.whitelist=*

#server
server.0=10.128.0.50:2888:3888
server.1=poc-2:2888:3888
server.2=poc-3:2888:3888
EOF

echo "0" >> /opt/zookeeper_data/myid    -->10.128.0.50
echo "1" >> /opt/zookeeper_data/myid    --> poc-2
echo "2" >> /opt/zookeeper_data/myid    -->poc-3


cp /opt/kafka/config/server.properties /opt/kafka/config/server.properties.backup

sed -i '/broker.id/d' /opt/kafka/config/server.properties
sed -i '/listeners=PLAINTEXT/d' /opt/kafka/config/server.properties
sed -i '/log.dirs=/d' /opt/kafka/config/server.properties
sed -i  '/num.partitions=/d' /opt/kafka/config/server.properties
sed -i '/offsets.topic.replication.factor=3/d' /opt/kafka/config/server.properties
sed -i '/transaction.state.log.replication.factor=3/d' /opt/kafka/config/server.properties
sed -i '/zookeeper.connect=/d' /opt/kafka/config/server.properties

#config for 10.128.0.50

cat >> /opt/kafka/config/server.properties << EOF
broker.id=1
listeners=PLAINTEXT://10.128.0.50:9092
log.dirs=/opt/kafka-data
num.partitions=8
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
zookeeper.connect=10.128.0.50:2181,poc-2:2181,poc-3:2181/kafka
EOF

# config  for poc-2

cat >> /opt/kafka/config/server.properties << EOF
broker.id=2
listeners=PLAINTEXT://poc-2:9092
log.dirs=/opt/kafka-data
num.partitions=8
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
zookeeper.connect=10.128.0.50:2181,poc-2:2181,poc-3:2181/kafka
EOF

# config for poc-3

cat >> /opt/kafka/config/server.properties << EOF
broker.id=3
listeners=PLAINTEXT://poc-3:9092
log.dirs=/opt/kafka-data
num.partitions=8
offsets.topic.replication.factor=2
transaction.state.log.replication.factor=2
zookeeper.connect=10.128.0.50:2181,poc-2:2181,poc-3:2181/kafka
EOF

# nohup bin/zookeeper-server-start.sh  config/zookeeper.properties & > /dev/null 2>&1
# nohup bin/kafka-server-start.sh config/server.properties & 
# echo "stat" | nc 10.128.0.50 2181   -->check stat

# bin/kafka-topics.sh --zookeeper 10.128.0.50:2181,poc-2:2181,poc-3:2181/kafka --create --topic first_topic --replication-factor 3 --partitions 3

# bin/kafka-console-producer.sh --broker-list 10.128.0.50:9092,poc-2:9092,poc-3:9092 --topic first_topic

# bin/kafka-console-consumer.sh --bootstrap-server 10.128.0.50:9092,poc-2:9092,poc-3:9092 --topic first_topic --from-beginning



=======================oozie==========

#https://www.cloudduggu.com/oozie/installation/ --> refered

wget http://archive.apache.org/dist/oozie/5.2.0/oozie-5.2.0.tar.gz
tar -xzf oozie-5.2.0.tar.gz
mv oozie-5.2.0 oozie

sudo yum -y install maven

cd /opt/oozie/bin
./mkdistro.sh -DskipTests

cd /opt/oozie/distro/target/oozie-5.2.0-distro/oozie-5.2.0
mkdir libext






